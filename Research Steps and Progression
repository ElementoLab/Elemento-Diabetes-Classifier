Initial thoughts:

* IT MAY BE BEST TO DOWNLOAD THE COLUMNS ONTO MY COMPUTER THAT ARE USED TO DEFINE WHO HAS PREDIABETES AND WHO PROGRESSES TO DIABETES. 
IT SHOULD ONLY BE A FEW COLUMNS SO IT WILL NOT TAKE UP A LOT OF ROOM ON MY COMPUTER AND IT SHOULD BE QUICK COMPUTATIONALLY. 
THEN I CAN LOOK AT THE DATA ON A JUPYTER NOTEBOOK AND IT WILL BE MUCH EASIER TO DO WHAT I WANT.

# Procedure and Thinking Throughout the Project

1. Unzipping UKBiobank files for use in python or other language for data manipulation

BEFORE MANIPULATION OF THE DATA, WE FIRST HAVE TO OPEN THE .gz FILES USING LINUX COMMAND zcat FOR ALL FILES USED ALONG WITH USING THE > COMMAND TO MAKE THIS INTO A USABLE FILE. 
AN EXAMPLE CODE FOR ukb26867 IS SHOWN BELOW (for my path to the ukb26867 file in the SCU):
zcat /athena/elementolab/scratch/nib4003/ukbiobank/phenotypes/ukb26867.csv.gz > whole_file_26867

* This does take a little bit of time since the files are so big, but then we can use them in python
****** VERY IMPORTANT: We noticed differences in the counts across HTML files which contain the information regarding the same data 
(e.g. 20002-X.X occurs in whole_file_26867 and whole_file_33822). 
Scott said that this was most likely due to patients withdrawing their information from the UKBiobank over time, and therefore, 
as ethicial researchers we must use the columns with less patients in our analyses. 
Therefore, we have to be careful to take only the patients who want to be included in the study 
and we must make sure to use any duplicate column with the least number of patients in it.


2. Setting up Python on the Scientific Computing Unit (SCU)

Below we show how to use Spack to load Python. Before this is possible, follow this documentation page to set up all necessities for spack: https://wcmscu.atlassian.net/wiki/spaces/WIKI/pages/33594/Spack

Load python 3.7.0:
spack load -r python@3.7.0^gcc@6.3.0

Shows that python is loaded:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

Load pandas for use in python:
spack load -r py-pandas

Do not need to run this line of code but it shows that pandas is downloaded now:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

To open python (very simple):
python

Now we can import pandas as usual in the newly opened python file:
import pandas as pd


3. Use Scientific Computing Unit to create dataframe for exportation to personal computer in order to classify prediabetic patients

The next step is to read in the columns we want for our dataframe to be able to diagnose prediabetes. 
We have to do this separately for the HTML files we need to use to diagnose prediabetes. 
which we will add to the doctor diagnosis of diabetes due to Scott's figure saying self and doctor diagnosis are not statistically different for diabetes.
The first step here contains all the information for doctor diagnosis, age of diagnosis, gestational diabetics, and dates the blood was drawn.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867', usecols = ['eid', '74-0.0', '74-1.0', '74-2.0', '2443-0.0', '2443-1.0', '2443-2.0', '2976-0.0', '2976-1.0', '2976-2.0', '4041-0.0', '4041-1.0', '4041-2.0', '53-0.0', '53-1.0', '53-2.0'])

The second step here contains all the columns that contain the tests used to diagnose a prediabetic patient - HbA1c and Blood Glucose.
HbA1c is a standard measure for the classification of prediabetic patients, with patients scoring 42 mmol/mol - 47 mmol/mol in the prediabetic range.
Blood Glucose can also be used to classify prediabetic patients. Here we assume that the blood glucose levels are after fasting and therefore the range for prediabetic patients
for fasting blood glucose level is 5.6 mmol/L - 7.0 mmol/L. 
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385', usecols = ['eid', '30740-0.0', '30740-1.0', '30741-0.0', '30741-1.0', '30750-0.0', '30750-1.0', '30751-0.0', '30751-1.0'])

The third step here contains all the columns that correspond to a self-diagnosis of diabetes. 
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822', usecols = ['eid', '20002-0.0','20002-0.1', '20002-0.2', '20002-0.3', '20002-0.4', '20002-0.5', '20002-0.6', '20002-0.7', '20002-0.8', '20002-0.9', '20002-0.10', '20002-0.11', '20002-0.12', '20002-0.13', '20002-0.14', '20002-0.15', '20002-0.16', '20002-0.17', '20002-0.18', '20002-0.19', '20002-0.20', '20002-0.21', '20002-0.22', '20002-0.23', '20002-0.24', '20002-0.25', '20002-0.26', '20002-0.27', '20002-0.28', '20002-0.29', '20002-0.30', '20002-0.31', '20002-0.32', '20002-0.33', '20002-1.0', '20002-1.1', '20002-1.2', '20002-1.3', '20002-1.4', '20002-1.5', '20002-1.6', '20002-1.7', '20002-1.8', '20002-1.9', '20002-1.10','20002-1.11', '20002-1.12', '20002-1.13', '20002-1.14', '20002-1.15', '20002-1.16', '20002-1.17', '20002-1.18', '20002-1.19', '20002-1.20', '20002-1.21', '20002-1.22', '20002-1.23', '20002-1.24', '20002-1.25', '20002-1.26', '20002-1.27', '20002-1.28', '20002-1.29', '20002-1.30', '20002-1.31', '20002-1.32', '20002-1.33', '20002-2.0', '20002-2.1', '20002-2.2', '20002-2.3', '20002-2.4', '20002-2.5', '20002-2.6', '20002-2.7', '20002-2.8', '20002-2.9', '20002-2.10','20002-2.11', '20002-2.12', '20002-2.13', '20002-2.14', '20002-2.15', '20002-2.16', '20002-2.17', '20002-2.18', '20002-2.19', '20002-2.20', '20002-2.21', '20002-2.22', '20002-2.23', '20002-2.24', '20002-2.25', '20002-2.26', '20002-2.27', '20002-2.28', '20002-2.29', '20002-2.30', '20002-2.31', '20002-2.32', '20002-2.33'])

Now we can merge these dataframes by eid (patient number).
merged_prediabetes_information = first_step.merge(second_step, on = 'eid').merge(third_step, on = 'eid')

Finally we can write this dataframe to a csv so that we can import it to our desktop using winscp.
merged_prediabetes_information.to_csv(path_or_buf = '~nib4003/for_winscp/merged_prediabetes_information')

Another thing we must look into is a different way of finding patients who develop diabetes after their prediabetic classification. 
This involves using the hesin files located in /athena/elementolab/scratch/nib4003/ukbiobank/hesin which includes two files we will use named 'hesin.txt' and 'hesin_diag.txt'. 
'hesin.txt' contains the dates when a patient is diagnosed with a disease, in our case of course diabetes. 
'hesin_diag.txt' contains the actual diagnosis of diabetes for patients using the code E11, which includes all values from E110-E119. 
E11 is defined as the diagnosis for non-insulin dependent diabetes which is type 2 diabetes. This can be found in data-coding 19 in UDI 41202-0.0. 
We can combine these files together to create one dataframe that holds all diagnoses of diabetes and the dates in which these diagnoses were made. 
We then compare the dates to those of the ones of UDI 53-0.0 which is the 
'Date of attneding assessment centre' column which contains the dates we use to diagnose prediabetic patients since this is when their samples were taken. 
If the date using the hesin text files is later than that of the csv file for a prediabetic patient, then this patient developed diabetes. 
If the diabetes diagnosis is before the date of the csv file for a prediabetic patient, 
then the patient is already classified as diabetic and must be removed from the dataframe. 
The code to create the dataframe with all dates from the hesin files is shown below.
To import 'hesin_diag.txt':
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

To find all diabetes diagnosed patients:
type2_diabetes_only = hesin_diag[hesin_diag['diag_icd10'].str.contains('E11',na = False)]

To import 'hesin.txt'"
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

Next we cut down the number of columns in order to have a merge of only the columns we need from 'hesin.txt'"
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']]

We merge the dataframes together by eid and ins_index to get the final result with the dates of diagnosis:
patients_with_type2_diabetes_with_dates = type2_diabetes_only.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])

Finally we have to save this dataframe as a csv to be exported to my computer so that we can use it to compare the dates in this file to the dates of the other file 
in order to see if more patients in a prediabetic state progressed to a diabetic state:
patients_with_type2_diabetes_with_dates.to_csv(path_or_buf = '~nib4003/for_winscp/patients_with_type2_diabetes_with_
dates')

* Notice how I have saved everything to a specific folder. I use this folder as a storage space to keep all files I move back and forth from my computer to the SCU
using WinSCP. If operating on a Windows computer, YOU SHOULD DOWNLOAD WinSCP found here: https://winscp.net/eng/download.php 
This tool makes transferring files from Windows to the SCU possible and very easy (drag and drop). 
Once the two csv files we created in step 3 are moved to the computer, upload them to Jupyter Notebook (what I did).
We then start the prediabetes classification and labeling of prediabetic patients who progressed to a diabetic state vs. those that never progressed to a diabetic state.

Below are the columns we kept in step 3 csv files:

In HTML: ukb42385.csv.gz

Glucose - 30740-0.0
Glucose - 30740-1.0
Glucose assay date - 30741-0.0
Glucose assay date - 30741.1.0
Glycated haemoglobin (HbA1c) - 30750-0.0
Glycated haemoglobin (HbA1c) - 30750-1.0
Glycated haemoglobin (HbA1c) assay date - 30751.0.0
Glycated haemoglobin (HbA1c) assay date - 30751.1.0
Date of attending assessment centre - 53-0.0
Date of attending assessment centre - 53-1.0
Date of attending assessment centre - 53-2.0

In HTML: ukb26867.csv.gz

Diabetes diagnosed by doctor - 2443-0.0
Diabetes diagnosed by doctor - 2443-1.0
Diabetes diagnosed by doctor - 2443-2.0
Age diabetes diagnosed - 2976-0.0
Age diabetes diagnosed - 2976-1.0
Age diabetes diagnosed - 2976-2.0
Gestational diabetes only - 4041-0.0
Gestational diabetes only - 4041-1.0
Gestational diabetes only - 4041-2.0
Fasting time - 74-0.0

In HTML: ukb33822.csv.gz

Non-cancer illness code, self-reported - Use all 20002-0.0 to 20002-2.33 (99 columns in total)


4. Prediabetic Classification and Progression to Diabetes Labeling - Look at Jupyter Notebook file "Prediabetes Classification.ipynb"

To begin, it would be most efficient to focus on finding the certain patients that will be involved in this study so that 
we do not have to waste time waiting for computations on many pateints to run initially. 
We do this using two measurements. First, we keep patients with an HbA1c measurement in the range 42 mmol/mol - 47 mmol/mol
Second, we use blood glucose to measure if a patient has prediabetes for patients who took a fasting blood sugar test.
For patients who fasted 8 or more hours, prediabetes based on blood glucose level lies in the range 5.6 mmol/L - 7.0 mmol/L
For patients who did not fast at least 8 hours, this test does not have a specified range for prediabetic patients, and therefore cannot be used.
We removed all doctor diagnosed diabetic patients who fall into our prediabetic ranges for the HbA1c and blood glucose measurements 
and also patients who have gestational diabetes only because these patients do not develop diabetes in the way we desire and may bias our model.

For our analysis, we assume that self-diagnosis of diabetes is equivalent to true diagnosis of diabetes.
Therefore, we need to take into account patients which we classify as prediabetic, but who self-diagnose themselves as diabetic at the start of the study.
These patients must be cut from the dataframe because we assume that they begin the study with diabetes, not prediabetes.
We also need to consider prediabetic patients who are never diagnosed with diabetes (through the doctor diagnosis columns 2443-X.X or through the ICD values),
but do self-diagnose themselves with diabetes. We need to keep these patients in the dataframe and classify them as prediabetes who progress to diabetes.

We use two sequential ideas to create the group of patients who will be kept for our classifier for non-self-diagnosis patients and how to label them:

The first is to use the ICD column in which prediabetic patients are diagnosed with diabetes after their initial prediabetic classification. 
This is done by comparing the "Date of attending assessment centre" (53-0.0) column from the ukb42385 csv file with the "epistart" column from the "hesin.txt" file.
Prediabetic classified patients with a "Date of attending assessment centre" before the "epistart" date are labeled as developing diabetes.
Prediabetic classified patients with an "epistart" date before the "Date of attending assessment centre" have diabetes before the actual prediabetic classification,
and must be cut from the patients we classified as prediabetic because they are truly classified as diabetics. 
Once we have found all patients who are shown to develop diabetes from the ICD codes 
(E110-E119 for Type 2 Diabetes shown here for Non-insulin-dependent diabetes mellitus - https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=19&nl=1),
we cut them from the total dataframe of prediabetic patients since we already know they are to be labeled as progressing to diabetes.

The second idea is to then filter through the remaining prediabetic patients 
and keep only the patients who we know either developed diabetes or did not develop diabetes at a later date.
To do this, we only keep prediabetic patients in our study that contain values in columns in which a doctor diagnosis whether a patient has diabetes or not 
at a later date when a patient returns and is re-evaluated (2443-1.0 and 2443-2.0). We must cut all the prediabetic classified patients from the final dataframe 
who do not have a confirmational value of whether or not they developed diabetes. We can then label the patients as developing diabetes 
if they begin classified as prediabetic (2443-0.0) but later are diagnosed with diabetes (2443-1.0 or 2443-2.0).

These two ideas allow us to label all our targets as progressing to diabetes or not. We then take a list of all eid numbers for prediabetic patients progressing 
to a diabetic state and label them. We also do the same for the remaining prediabetic patients who did not progress to a diabetic state.
We can then save our final dataframe to a csv and export it to the SCU using WinSCP where we begin the next step.


5. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 
First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
less_features_first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
less_features_first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
less_features_second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
less_features_third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
less_features_fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (5003, 12711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only')

6. Keeping Features For Final Model By Inspection - Look at Jupyter Notebook file "Machine Learning Start Using Model With All Features.ipynb"

We now have a dataframe containing all 5003 patients who began with prediabetes and either progressed to a diabetic state (4261) or did not progress to diabetes (742) 
who are described by a total of 12711 features. We notice that there are duplicate columns within the UKBiobank data which may be due to patients deciding to withdraw
their information from the database after their initial measurements. To be ethical researchers, we keep the columns which contain less information so as to not exploit
these patients. 

We then cut all features from the dataframe which contain only NaN values since these will not give any information for classification. This totals 3224 columns.
The next step in the pipeline is to cut all dates out of the dataframe. Since we trying to predict the patients who will and will not progress to a diabetic state
based on their initial information, the dates involved do not provide the model with information about the outcome of the patient. 
There are 54 different dates in the dataframe, some containing multiple entries. 

The next thing we do is look at the first column for each feature (those that end in -0.0) for a large proportion of NaN values. 
The reason we do this is because these columns mostly contain more information than the next numerical columns (-0.1 or -1.0) 
and therefore if there are many NaN values in the -0.0 columns then there will not be enough information in total to have any weight on our model.
We use a criteria of 80% NaN values or greater in a column to drop it from the dataframe. This results in a dataframe with 3901 columns.

Our next step is to understand what all remaining columns represent and filter out those that are not important.
The dataframe is composed of 3140 floats, 6 integers, and 755 objects.
We begin our filtering with the object features. All features are labeled in the .ipynb notebook and labeled as to whether or not they will be kept for the next steps.
We then drop columns which we believe have no relevance to the model's predictive ability. This results in a total of 3366 columns.
We next look at the integer features. We again show the features in the .ipynb notebook and decide which to keep. We end up keeping e of the 6 integer columns (eid included).
We finally look at the float features. In the .ipynb notebook we decide which to keep in our final model by expecting them in the HTML and on the UKBiobank. 
After dropping the unnecessary float features, we are left with a dataframe with 1373 columns.

Since we are to only focus on the initial data taken from the prediabetic patients, we drop all columns that do not end in X-0.X as these represent the recordings from
the first visit to the assessment center by the patients. 
However, we notice that there are some columns that are represented by one measurement in the form X-2.0, which we keep in the dataframe as well. 
The resulting number of features remaining is 899.

At this point, we further cut down on features with many NaN values. All columns with 30% or greater NaN values are dropped from the dataframe. The result is 217 columns.
The makeup of these data types is 205 floats, 3 integer, and 9 objects. 

Next we focus our attention on the remaining object columns. We believe that the only way to get information from these columns is to one hot encode the data 
and we do not believe this will actually result in a greater prediction power of the model because there are numerous values that are taken by all patients.
Values are not common among patients so the result would just be the creation of an enormous amount of columns that contain no information that helps us. 
Therefore, we drop the object columns from the dataframe.

We again label all remaining integer and float features. We find that some of our floats are categorical in nature and therefore we must one hot encode these columns using 
pandas get_dummies. The NaN values remain in these columns when using this function.

We notice that some of the float columns contain negatives, which our model does not allow for. 
We therefore need to make a coding for these patients which rectifies the situation. All codings for all categorical floats are found in the .ipynb. 
When we set the values of -1 and -3 to NaN, these will be filled in like any other NaN value with the median of the column.

Next, we fill all NaN values in the remaining float columns with the median of each column, so as to not bias the center of each data distribution.

We then read in the dataframe with each patient labeled correctly from "Prediabetes Classification.ipynb" and merge the target column with our current dataframe.

After further inspection, we find that there are columns in our dataframe which contain only 0 values and therefore will not increase the model's prediction power.
Therefore, we cut these columns from the dataframe, resulting in a dataframe with 479 columns after one hot encoding.

Next, we normalize all remaining float columns. First, for all columns containing negative values, we subtract the minimum from all values (resulting in adding the 
absolute value of the most negative number), resulting in only positive numbers. We then normalized all float columns by dividing each column by its maximum
value, effectively resulting in the range of each column to be between 0 and 1. 

We then saved this dataframe in csv format to be used in another notebook for further feature selection and machine learning model creation.


