Initial thoughts:

* IT MAY BE BEST TO DOWNLOAD THE COLUMNS ONTO MY COMPUTER THAT ARE USED TO DEFINE WHO HAS PREDIABETES AND WHO PROGRESSES TO DIABETES. 
IT SHOULD ONLY BE A FEW COLUMNS SO IT WILL NOT TAKE UP A LOT OF ROOM ON MY COMPUTER AND IT SHOULD BE QUICK COMPUTATIONALLY. 
THEN I CAN LOOK AT THE DATA ON A JUPYTER NOTEBOOK AND IT WILL BE MUCH EASIER TO DO WHAT I WANT.

# Procedure and Thinking Throughout the Project

1. Unzipping UKBiobank files for use in python or other language for data manipulation

BEFORE MANIPULATION OF THE DATA, WE FIRST HAVE TO OPEN THE .gz FILES USING LINUX COMMAND zcat FOR ALL FILES USED ALONG WITH USING THE > COMMAND TO MAKE THIS INTO A USABLE FILE. 
AN EXAMPLE CODE FOR ukb26867 IS SHOWN BELOW (for my path to the ukb26867 file in the SCU):
zcat /athena/elementolab/scratch/nib4003/ukbiobank/phenotypes/ukb26867.csv.gz > whole_file_26867

* This does take a little bit of time since the files are so big, but then we can use them in python
****** VERY IMPORTANT: We noticed differences in the counts across HTML files which contain the information regarding the same data 
(e.g. 20002-X.X occurs in whole_file_26867 and whole_file_33822). 
Scott said that this was most likely due to patients withdrawing their information from the UKBiobank over time, and therefore, 
as ethicial researchers we must use the columns with less patients in our analyses. 
Therefore, we have to be careful to take only the patients who want to be included in the study 
and we must make sure to use any duplicate column with the least number of patients in it.


2. Setting up Python on the Scientific Computing Unit (SCU)

Below we show how to use Spack to load Python. Before this is possible, follow this documentation page to set up all necessities for spack: https://wcmscu.atlassian.net/wiki/spaces/WIKI/pages/33594/Spack

Load python 3.7.0:
spack load -r python@3.7.0^gcc@6.3.0

Shows that python is loaded:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

Load pandas for use in python:
spack load -r py-pandas

Do not need to run this line of code but it shows that pandas is downloaded now:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

To open python (very simple):
python

Now we can import pandas as usual in the newly opened python file:
import pandas as pd


3. Use Scientific Computing Unit to create dataframe for exportation to personal computer in order to classify prediabetic patients

The next step is to read in the columns we want for our dataframe to be able to diagnose prediabetes. 
We have to do this separately for the HTML files we need to use to diagnose prediabetes. 
which we will add to the doctor diagnosis of diabetes due to Scott's figure saying self and doctor diagnosis are not statistically different for diabetes.
The first step here contains all the information for doctor diagnosis, age of diagnosis, gestational diabetics, and dates the blood was drawn.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867', usecols = ['eid', '74-0.0', '74-1.0', '74-2.0', '2443-0.0', '2443-1.0', '2443-2.0', '2976-0.0', '2976-1.0', '2976-2.0', '4041-0.0', '4041-1.0', '4041-2.0', '53-0.0', '53-1.0', '53-2.0'])

The second step here contains all the columns that contain the tests used to diagnose a prediabetic patient - HbA1c and Blood Glucose.
HbA1c is a standard measure for the classification of prediabetic patients, with patients scoring 42 mmol/mol - 47 mmol/mol in the prediabetic range.
Blood Glucose can also be used to classify prediabetic patients. Here we assume that the blood glucose levels are after fasting and therefore the range for prediabetic patients
for fasting blood glucose level is 5.6 mmol/L - 7.0 mmol/L. 
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385', usecols = ['eid', '30740-0.0', '30740-1.0', '30741-0.0', '30741-1.0', '30750-0.0', '30750-1.0', '30751-0.0', '30751-1.0'])

The third step here contains all the columns that correspond to a self-diagnosis of diabetes. 
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822', usecols = ['eid', '20002-0.0','20002-0.1', '20002-0.2', '20002-0.3', '20002-0.4', '20002-0.5', '20002-0.6', '20002-0.7', '20002-0.8', '20002-0.9', '20002-0.10', '20002-0.11', '20002-0.12', '20002-0.13', '20002-0.14', '20002-0.15', '20002-0.16', '20002-0.17', '20002-0.18', '20002-0.19', '20002-0.20', '20002-0.21', '20002-0.22', '20002-0.23', '20002-0.24', '20002-0.25', '20002-0.26', '20002-0.27', '20002-0.28', '20002-0.29', '20002-0.30', '20002-0.31', '20002-0.32', '20002-0.33', '20002-1.0', '20002-1.1', '20002-1.2', '20002-1.3', '20002-1.4', '20002-1.5', '20002-1.6', '20002-1.7', '20002-1.8', '20002-1.9', '20002-1.10','20002-1.11', '20002-1.12', '20002-1.13', '20002-1.14', '20002-1.15', '20002-1.16', '20002-1.17', '20002-1.18', '20002-1.19', '20002-1.20', '20002-1.21', '20002-1.22', '20002-1.23', '20002-1.24', '20002-1.25', '20002-1.26', '20002-1.27', '20002-1.28', '20002-1.29', '20002-1.30', '20002-1.31', '20002-1.32', '20002-1.33', '20002-2.0', '20002-2.1', '20002-2.2', '20002-2.3', '20002-2.4', '20002-2.5', '20002-2.6', '20002-2.7', '20002-2.8', '20002-2.9', '20002-2.10','20002-2.11', '20002-2.12', '20002-2.13', '20002-2.14', '20002-2.15', '20002-2.16', '20002-2.17', '20002-2.18', '20002-2.19', '20002-2.20', '20002-2.21', '20002-2.22', '20002-2.23', '20002-2.24', '20002-2.25', '20002-2.26', '20002-2.27', '20002-2.28', '20002-2.29', '20002-2.30', '20002-2.31', '20002-2.32', '20002-2.33'])

Now we can merge these dataframes by eid (patient number).
merged_prediabetes_information = first_step.merge(second_step, on = 'eid').merge(third_step, on = 'eid')

Finally we can write this dataframe to a csv so that we can import it to our desktop using winscp.
merged_prediabetes_information.to_csv(path_or_buf = '~nib4003/for_winscp/merged_prediabetes_information')

Another thing we must look into is a different way of finding patients who develop diabetes after their prediabetic classification. 
This involves using the hesin files located in /athena/elementolab/scratch/nib4003/ukbiobank/hesin which includes two files we will use named 'hesin.txt' and 'hesin_diag.txt'. 
'hesin.txt' contains the dates when a patient is diagnosed with a disease, in our case of course diabetes. 
'hesin_diag.txt' contains the actual diagnosis of diabetes for patients using the code E11, which includes all values from E110-E119. 
E11 is defined as the diagnosis for non-insulin dependent diabetes which is type 2 diabetes. This can be found in data-coding 19 in UDI 41202-0.0. 
We can combine these files together to create one dataframe that holds all diagnoses of diabetes and the dates in which these diagnoses were made. 
We then compare the dates to those of the ones of UDI 53-0.0 which is the 
'Date of attneding assessment centre' column which contains the dates we use to diagnose prediabetic patients since this is when their samples were taken. 
If the date using the hesin text files is later than that of the csv file for a prediabetic patient, then this patient developed diabetes. 
If the diabetes diagnosis is before the date of the csv file for a prediabetic patient, 
then the patient is already classified as diabetic and must be removed from the dataframe. 
The code to create the dataframe with all dates from the hesin files is shown below.
To import 'hesin_diag.txt':
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

To find all diabetes diagnosed patients:
type2_diabetes_only = hesin_diag[hesin_diag['diag_icd10'].str.contains('E11',na = False)]

To import 'hesin.txt'"
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

Next we cut down the number of columns in order to have a merge of only the columns we need from 'hesin.txt'"
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']]

We merge the dataframes together by eid and ins_index to get the final result with the dates of diagnosis:
patients_with_type2_diabetes_with_dates = type2_diabetes_only.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])

Finally we have to save this dataframe as a csv to be exported to my computer so that we can use it to compare the dates in this file to the dates of the other file 
in order to see if more patients in a prediabetic state progressed to a diabetic state:
patients_with_type2_diabetes_with_dates.to_csv(path_or_buf = '~nib4003/for_winscp/patients_with_type2_diabetes_with_
dates')

* Notice how I have saved everything to a specific folder. I use this folder as a storage space to keep all files I move back and forth from my computer to the SCU
using WinSCP. If operating on a Windows computer, YOU SHOULD DOWNLOAD WinSCP found here: https://winscp.net/eng/download.php 
This tool makes transferring files from Windows to the SCU possible and very easy (drag and drop). 
Once the two csv files we created in step 3 are moved to the computer, upload them to Jupyter Notebook (what I did).
We then start the prediabetes classification and labeling of prediabetic patients who progressed to a diabetic state vs. those that never progressed to a diabetic state.

Below are the columns we kept in step 3 csv files:

In HTML: ukb42385.csv.gz

Glucose - 30740-0.0
Glucose - 30740-1.0
Glucose assay date - 30741-0.0
Glucose assay date - 30741.1.0
Glycated haemoglobin (HbA1c) - 30750-0.0
Glycated haemoglobin (HbA1c) - 30750-1.0
Glycated haemoglobin (HbA1c) assay date - 30751.0.0
Glycated haemoglobin (HbA1c) assay date - 30751.1.0
Date of attending assessment centre - 53-0.0
Date of attending assessment centre - 53-1.0
Date of attending assessment centre - 53-2.0

In HTML: ukb26867.csv.gz

Diabetes diagnosed by doctor - 2443-0.0
Diabetes diagnosed by doctor - 2443-1.0
Diabetes diagnosed by doctor - 2443-2.0
Age diabetes diagnosed - 2976-0.0
Age diabetes diagnosed - 2976-1.0
Age diabetes diagnosed - 2976-2.0
Gestational diabetes only - 4041-0.0
Gestational diabetes only - 4041-1.0
Gestational diabetes only - 4041-2.0
Fasting time - 74-0.0

In HTML: ukb33822.csv.gz

Non-cancer illness code, self-reported - Use all 20002-0.0 to 20002-2.33 (99 columns in total)


4. Prediabetic Classification and Progression to Diabetes Labeling - Look at Jupyter Notebook file "Model 1, 2 - Prediabetes Classification.ipynb"

To begin, it would be most efficient to focus on finding the certain patients that will be involved in this study so that 
we do not have to waste time waiting for computations on many pateints to run initially. 
We do this using two measurements. First, we keep patients with an HbA1c measurement in the range 42 mmol/mol - 47 mmol/mol
Second, we use blood glucose to measure if a patient has prediabetes for patients who took a fasting blood sugar test.
For patients who fasted 8 or more hours, prediabetes based on blood glucose level lies in the range 5.6 mmol/L - 7.0 mmol/L
For patients who did not fast at least 8 hours, this test does not have a specified range for prediabetic patients, and therefore cannot be used.
We removed all doctor diagnosed diabetic patients who fall into our prediabetic ranges for the HbA1c and blood glucose measurements 
and also patients who have gestational diabetes only because these patients do not develop diabetes in the way we desire and may bias our model.

For our analysis, we assume that self-diagnosis of diabetes is equivalent to true diagnosis of diabetes.
Therefore, we need to take into account patients which we classify as prediabetic, but who self-diagnose themselves as diabetic at the start of the study.
These patients must be cut from the dataframe because we assume that they begin the study with diabetes, not prediabetes.
We also need to consider prediabetic patients who are never diagnosed with diabetes (through the doctor diagnosis columns 2443-X.X or through the ICD values),
but do self-diagnose themselves with diabetes. We need to keep these patients in the dataframe and classify them as prediabetes who progress to diabetes.

We use two sequential ideas to create the group of patients who will be kept for our classifier for non-self-diagnosis patients and how to label them:

The first is to use the ICD column in which prediabetic patients are diagnosed with diabetes after their initial prediabetic classification. 
This is done by comparing the "Date of attending assessment centre" (53-0.0) column from the ukb42385 csv file with the "epistart" column from the "hesin.txt" file.
Prediabetic classified patients with a "Date of attending assessment centre" before the "epistart" date are labeled as developing diabetes.
Prediabetic classified patients with an "epistart" date before the "Date of attending assessment centre" have diabetes before the actual prediabetic classification,
and must be cut from the patients we classified as prediabetic because they are truly classified as diabetics. 
Once we have found all patients who are shown to develop diabetes from the ICD codes 
(E110-E119 for Type 2 Diabetes shown here for Non-insulin-dependent diabetes mellitus - https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=19&nl=1),
we cut them from the total dataframe of prediabetic patients since we already know they are to be labeled as progressing to diabetes.

The second idea is to then filter through the remaining prediabetic patients 
and keep only the patients who we know either developed diabetes or did not develop diabetes at a later date.
To do this, we only keep prediabetic patients in our study that contain values in columns in which a doctor diagnosis whether a patient has diabetes or not 
at a later date when a patient returns and is re-evaluated (2443-1.0 and 2443-2.0). We must cut all the prediabetic classified patients from the final dataframe 
who do not have a confirmational value of whether or not they developed diabetes. We can then label the patients as developing diabetes 
if they begin classified as prediabetic (2443-0.0) but later are diagnosed with diabetes (2443-1.0 or 2443-2.0).

These two ideas allow us to label all our targets as progressing to diabetes or not. We then take a list of all eid numbers for prediabetic patients progressing 
to a diabetic state and label them. We also do the same for the remaining prediabetic patients who did not progress to a diabetic state.
We can then save our final dataframe to a csv and export it to the SCU using WinSCP where we begin the next step.


5. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 
First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (5003, 12711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only')


6. Keeping Features For Final Model By Inspection - Look at Jupyter Notebook file "Model 1 - Machine Learning Start Using Model With All Features.ipynb"

We now have a dataframe containing all 5003 patients who began with prediabetes and either progressed to a diabetic state (4261) or did not progress to diabetes (742) 
who are described by a total of 12711 features. We notice that there are duplicate columns within the UKBiobank data which may be due to patients deciding to withdraw
their information from the database after their initial measurements. To be ethical researchers, we keep the columns which contain less information so as to not exploit
these patients. 

We then cut all features from the dataframe which contain only NaN values since these will not give any information for classification. This totals 3224 columns.
The next step in the pipeline is to cut all dates out of the dataframe. Since we trying to predict the patients who will and will not progress to a diabetic state
based on their initial information, the dates involved do not provide the model with information about the outcome of the patient. 
There are 54 different dates in the dataframe, some containing multiple entries. 

The next thing we do is look at the first column for each feature (those that end in -0.0) for a large proportion of NaN values. 
The reason we do this is because these columns mostly contain more information than the next numerical columns (-0.1 or -1.0) 
and therefore if there are many NaN values in the -0.0 columns then there will not be enough information in total to have any weight on our model.
We use a criteria of 80% NaN values or greater in a column to drop it from the dataframe. This results in a dataframe with 3901 columns.

Our next step is to understand what all remaining columns represent and filter out those that are not important.
The dataframe is composed of 3140 floats, 6 integers, and 755 objects.
We begin our filtering with the object features. All features are labeled in the .ipynb notebook and labeled as to whether or not they will be kept for the next steps.
We then drop columns which we believe have no relevance to the model's predictive ability. This results in a total of 3366 columns.
We next look at the integer features. We again show the features in the .ipynb notebook and decide which to keep. We end up keeping e of the 6 integer columns (eid included).
We finally look at the float features. In the .ipynb notebook we decide which to keep in our final model by expecting them in the HTML and on the UKBiobank. 
After dropping the unnecessary float features, we are left with a dataframe with 1373 columns.

Since we are to only focus on the initial data taken from the prediabetic patients, we drop all columns that do not end in X-0.X as these represent the recordings from
the first visit to the assessment center by the patients. 
However, we notice that there are some columns that are represented by one measurement in the form X-2.0, which we keep in the dataframe as well. 
The resulting number of features remaining is 899.

At this point, we further cut down on features with many NaN values. All columns with 30% or greater NaN values are dropped from the dataframe. The result is 217 columns.
The makeup of these data types is 205 floats, 3 integer, and 9 objects. 

Next we focus our attention on the remaining object columns. We believe that the only way to get information from these columns is to one hot encode the data 
and we do not believe this will actually result in a greater prediction power of the model because there are numerous values that are taken by all patients.
Values are not common among patients so the result would just be the creation of an enormous amount of columns that contain no information that helps us. 
Therefore, we drop the object columns from the dataframe.

We again label all remaining integer and float features. We find that some of our floats are categorical in nature and therefore we must one hot encode these columns using 
pandas get_dummies. The NaN values remain in these columns when using this function.

We notice that some of the float columns contain negatives, which our model does not allow for. 
We therefore need to make a coding for these patients which rectifies the situation. All codings for all categorical floats are found in the .ipynb. 
When we set the values of -1 and -3 to NaN, these will be filled in like any other NaN value with the median of the column.

Next, we fill all NaN values in the remaining float columns with the median of each column, so as to not bias the center of each data distribution.

We then read in the dataframe with each patient labeled correctly from "Prediabetes Classification.ipynb" and merge the target column with our current dataframe.

After further inspection, we find that there are columns in our dataframe which contain only 0 values and therefore will not increase the model's prediction power.
Therefore, we cut these columns from the dataframe, resulting in a dataframe with 479 columns after one hot encoding.

Next, we normalize all remaining float columns. First, for all columns containing negative values, we subtract the minimum from all values (resulting in adding the 
absolute value of the most negative number), resulting in only positive numbers. We then normalized all float columns by dividing each column by its maximum
value, effectively resulting in the range of each column to be between 0 and 1. 

We then saved this dataframe in csv format to be used in another notebook for further feature selection and machine learning model creation.


7. Creation of a dataframe using "Episodes" features from ukb26867 and Analysis of Model - Look at 
"Model 1 - Machine Learning After Inspection of All Features INCLUDING Episodes" and 
"Model 1 - Feature Selection and Machine Learning After Inspection of All Features With and Without prs" (contains models without Episodes features 
even though this notebook has been updated such that the analysis shows the addition of the Episodes columns before the models constructed at the end of the notebook) and
"Model 1 - Parameter Tuning for Model With Episodes Features"

We have gone back to the model and found that there are columns which begin with "Episodes" which we want to keep in the model because we believe they contain 
information that can help the predictive power of the model. We redid the analysis in the "Machine Learning After Inspection of All Features" notebook and saved 
this dataframe as 'prediabetes_df_after_data_manipulation_ready_for_feature_selection_with_episodes_data' to distinguish it from the dataframe we saved without the episodes
columns entitled 'prediabetes_df_after_data_manipulation_ready_for_feature_selection'. In the "Machine Learning After Inspection of All Features" notebook, the models at 
the end of this notebook use the 'prediabetes_df_after_data_manipulation_ready_for_feature_selection' dataframe, not the updated one that was created in this notebook. 
In the notebook entited "Machine Learning After Inspection of All Features INCLUDING Episodes", we have made models using the 
'prediabetes_df_after_data_manipulation_ready_for_feature_selection_with_episodes_data' dataframe and have shown that these features increase the AUC ROC when incorporated.

The notebook "Parameter Tuning for Model With Episodes Features" contains a section in which we attempt to change the parameters of the Random Forest Classifier in order 
to receive better metrics. We do this by changing the values of many of the hyperparameters for the model and observing the changes in the ROC AUC and confusion matrices
for cross fold validation. Our results show that the default parameters seem to work optimally for our model.


8a. Adding ICD Codes Step 1 - Getting ICD Codes for Prediabetic Patients From SCU

One thing we want to try to incorporate into the model is all the different diseases every prediabetic patient has. One way in which we believe this is possible is to make 
a count of all the different diagnoses for a patient and use this as an extra feature. To do so, we must use the hesin.txt and hesin_diag.txt files again.
Our commands are very similar to those described in 3., only this time we are keeping all diagnoses instead of just type 2 diabetes. The commands used are shown below.
* Note we will be using python so use the information above in 2. to open python for the following use.

First, we import the hesin_diag.txt file:
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

Next, we keep only the columns that will help us make the counts of diagnoses for each patient:
necessary_columns_for_diagnosis = hesin_diag[['eid', 'ins_index', 'diag_icd9', 'diag_icd10']] 

The next step is to import the hesin.txt file:
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

We then cut down the columns for this file to those that we want to keep:
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']] 

Next we merge the two dataframes together by eid and ins_index to match the correct patients with the correct disease:
all_patients_and_their_diagnoses = necessary_columns_for_diagnosis.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])  

Since this results in a large number of rows, we want to keep only the prediabetic patient results. Therefore, we import our prediabetic dataframe.
all_prediabetes = pd.read_csv('/home/nib4003/for_winscp/prediabetic_all_possible_to_classify_final')

We have to drop an unncessary column that got dragged along:
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Below we make a list of the eid numbers for prediabetic patients:
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we can keep only the prediabetic patients from the total dataframe:
all_diagnoses_only_prediabetics_at_start = all_patients_and_their_diagnoses[all_patients_and_their_diagnoses.eid.isin(dfToList_all_prediabetes)]

Finally, we save this dataframe for transfer to the SCU:
all_diagnoses_only_prediabetics_at_start.to_csv(path_or_buf = '~nib4003/for_winscp/only_prediabetes_patients_all_diagnoses_for_every_disease')


8b. Adding ICD Codes Step 2 - Applying ICD Codes To Prediabetic Patients - Look at "ICD Column Incorporation" Notebook

In this notebook, we add the ICD codes to our model which begins using all features of the total dataframe in order to understand how multiple different diagnoses can 
effect a patient. We want to see if more diagnoses increases the prediction power of the model that a prediabetic patient will or will not progress to a diabetic state. 
This is further explained later on in step 13.


9. Prediabetic Classification and Progression to Diabetes Labeling With Assumption That All Patients Who Have Diabetes Are Explicitly Labeled (Using ICD Codes Or Diabetes
Diagnosed by Doctor Columns Directly From UKBiobank) - Look at Jupyter Notebook file "Model 3 - Prediabetic Classification keeping ALL Prediabetic Patients.ipynb"

To begin, it would be most efficient to focus on finding the certain patients that will be involved in this study so that 
we do not have to waste time waiting for computations on many pateints to run initially. 
We do this using two measurements. First, we keep patients with an HbA1c measurement in the range 42 mmol/mol - 47 mmol/mol
Second, we use blood glucose to measure if a patient has prediabetes for patients who took a fasting blood sugar test.
For patients who fasted 8 or more hours, prediabetes based on blood glucose level lies in the range 5.6 mmol/L - 7.0 mmol/L
For patients who did not fast at least 8 hours, this test does not have a specified range for prediabetic patients, and therefore cannot be used.
We removed all doctor diagnosed diabetic patients who fall into our prediabetic ranges for the HbA1c and blood glucose measurements 
and also patients who have gestational diabetes only because these patients do not develop diabetes in the way we desire and may bias our model.

For our analysis, we assume that self-diagnosis of diabetes is equivalent to true diagnosis of diabetes.
Therefore, we need to take into account patients which we classify as prediabetic, but who self-diagnose themselves as diabetic at the start of the study.
These patients must be cut from the dataframe because we assume that they begin the study with diabetes, not prediabetes.
We also need to consider prediabetic patients who are never diagnosed with diabetes (through the doctor diagnosis columns 2443-X.X or through the ICD values),
but do self-diagnose themselves with diabetes. We need to keep these patients in the dataframe and classify them as prediabetes who progress to diabetes.

We use two sequential ideas to create the group of patients who will be kept for our classifier for non-self-diagnosis patients and how to label them:

The first is to use the ICD column in which prediabetic patients are diagnosed with diabetes after their initial prediabetic classification. 
This is done by comparing the "Date of attending assessment centre" (53-0.0) column from the ukb42385 csv file with the "epistart" column from the "hesin.txt" file.
Prediabetic classified patients with a "Date of attending assessment centre" before the "epistart" date are labeled as developing diabetes.
Prediabetic classified patients with an "epistart" date before the "Date of attending assessment centre" have diabetes before the actual prediabetic classification,
and must be cut from the patients we classified as prediabetic because they are truly classified as diabetics. 
Once we have found all patients who are shown to develop diabetes from the ICD codes 
(E110-E119 for Type 2 Diabetes shown here for Non-insulin-dependent diabetes mellitus - https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=19&nl=1),
we cut them from the total dataframe of prediabetic patients since we already know they are to be labeled as progressing to diabetes.

The second idea is to then filter through the remaining prediabetic patients 
and keep only the patients who we know either developed diabetes or did not develop diabetes at a later date.
To do this, we only keep prediabetic patients in our study that contain values in columns in which a doctor diagnosis whether a patient has diabetes or not 
at a later date when a patient returns and is re-evaluated (2443-1.0 and 2443-2.0). We must cut all the prediabetic classified patients from the final dataframe 
who do not have a confirmational value of whether or not they developed diabetes. We can then label the patients as developing diabetes 
if they begin classified as prediabetic (2443-0.0) but later are diagnosed with diabetes (2443-1.0 or 2443-2.0).

These two ideas allow us to label all our targets as progressing to diabetes or not. We then take a list of all eid numbers for prediabetic patients progressing 
to a diabetic state and label them. We also do the same for the remaining prediabetic patients who did not progress to a diabetic state.
We can then save our final dataframe to a csv and export it to the SCU using WinSCP where we begin the next step.


10. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection for the sample
of patients which we assume do not progress to diabetes if not explicitly stated through ICD or doctor diagnosis of diabetes from UKBiobank columns directly.

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 
First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_keeping_all_prediabetic_patients_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
less_features_first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (17,200, 12,711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only_keeping_all_prediabetic_patients')

*Notice the only change between this and step 5. is the importing of the correct dataframe to being and the export of the final dataframe with a new name including 
"keeping_all_prediabetic_".


11. Testing Model Created Using the Sample Of Patients Which We Assume Do Not Progress to Diabetes If Not Explicitly Stated Through ICD Or 
Doctor Diagnosis Of Diabetes From UKBiobank Columns Directly - Look at Notebook "Model 3 - Machine Learning Start Using Model With All Features keeping ALL prediabetic patients.ipynb"

In this notebook, we run through the same analysis previously performed in "Machine Learning Start Using Model With All Features.ipynb" but this time we had a larger 
sample size due to our assumption that prediabetic patients who are not classified as diabetic remain prediabetic. We show in this notebook that even though we have a much
larger sample size, our results are actually worse than the results without the assumption. Therefore, we find that more data is not as good as quality data. 
We do not further our analysis with this assumption due to its worse performance. 


12. Testing Using Many Machine Learning Algorithms - Look at Notebook "Model 1 - All Models Matt Used Applied"

After Matt Wickersham's great presentation, we decided to follow his steps and use all ML algorithms he used for his classification of alcohol usage. We also added the 
algorithms Complement Naive Bayes, Linear SVM, and Easy Ensemble. 

In this notebook, we use stratified k fold cross validation, due to the imabalance between our cohort group sample sizes, with the number of splits equal to 10. 
For all ML algorithms, we calculate the accuracy, area under the receiver operating characteristic curve, area under the precision recall curve, f1 scores, 
and all corresponding standard deviations to show as metrics for the classification of prediabetic patients progressing to a diabetic state. 
We also plot an ROC AUC curve corresponding to every algorithm together to compare the results of the different models for this metric.
We also plot a precision recall curve corresponding to every algorithm together to compare the results of the different models for this metric.
Tables of these values have been created using Word to show all results in the same place. 


13. Incorporating New Features Using ICD Codes for Patients With Hypertension or Obesity and The Number of Diseases - Look at Notebook "ICD Column Incorporation.ipynb"

In this notebook, we use the ICD10 column from the hesin files in the SCU to create two features which we add to the model which was created using all features to begin. 

1. The first feature is the number of diseases a patient has before their initial measurments at the assessment center. We count all the diseases a patient is diagnosed
with that have an epistart date before the date of attending assessment center column. We then add these values into the overall dataframe with all the features.
For patients who do not have a reported medication being taken, we give them a value of 0.

2. The second feature is the labeling of whether a patient had obesity and/or hypertension before the initial measurements were taken, once again comparing the epistart 
and date of attending assessment center columns. We believe that the prediabetic patients who had obesity and/or hypertension would most likely develop diabetes and 
would provide the model with a better prediction power. 

Unfortunately, after incorporation of these features into our model, we found that their feature importances were extremely low and had no impact on the model. Therefore,
we decide not to use them as features in future models.


14. Running Statistical Tests Between Different Cohort Features - Look at Notebook "Model 1, 2 - Demographic and Other Information for Different Cohorts"

In this notebook, we recreated Matt Wickersham's tables for his project which contain different demographic and medical information for the two cohorts. 
We calculate the average values of many features in the model, and run Levene's test on all continous variables first to determine if the variance of the two groups differ
in respect to any of the features. We find that this is the case only for the age of the patients between the two cohorts. We then used independent t-tests to statistically
show if there is a significant difference between the means of the features we included for cohorts. We discovered that the only feature with a statistically significant 
difference between the means is Age. 


15. Comparison of Model With and Without HbA1c and Glucose Features - Look at Notebooks 
"Model 1 - ML Model Feature Selection WITH HBA1C and GLUCOSE.ipynb" and "Model 1 - Feature Selection and Machine Learning After Inspection of All Features With and Without prs.ipynb"

The first notebook contains a model with HbA1c and Glucose, while the second notebook does not contain these features. We see that the model that does contain them 
performs better, and HbA1c is even a top feature. We believe that even though we used this to define our patients who are prediabetic, this feature is still necessary 
to keep in the model and there is no bias resulting from leaving it in the model.


16. Creation of New Model By Adding New Features From Literature to Previous Model With Only Features From Literature - Look at 
Notebook "Model 2.1 - Machine Learning Start Features From Literature With More Features Added" - 
contains additional features from https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0918-5

In this notebook, we add new features from the source above. This paper used machine learning models to predict prediabetic patients from non-prediabetic patients.
Although this is not the same task as us, they provide what could be used as a baseline for comparison with our model's metrics. This paper also discusses other papers and 
lists some of the features from those publications, which this lab group also inclued in their models. 

We show that with the inclusion of the new features in our model, the ROC AUC is greater than that of the first model created without the features. However, the performance
is not as good as the model which began using all features. We also repeat the analysis in "All Models Matt Used Applied " to get the metrics for all machine learning 
algorithms. A table was created which contains the accuracy, AUC ROC, AUPRC, F1 score, and the uncertainties for all metrics for classifying prediabetic patients 
progressing to a diabetic state.


17. Creation of Models Using Patients Who Progress To A Diabetic State WIthin Certain Time Frames Of Prediabetic Classification Based On Their Attending Assessment Center Date 
and all their demographic cohort comparisons - Look at Notebooks "Model 4.1 - Model Using Patients Who Progress to Diabetes Within One Year" and 
"Model 4.2 - Model Using Patients Who Progress to Diabetes Within Two Years"
and "Model 4.3 - Model Using Patients Who Progress to Diabetes Within Three Years" and "Model 4.4 - Model Using Patients Who Progress to Diabetes Within Four Years" and
"Model 4.4a - Model Using Patients Who Progress to Diabetes Within Four Years Without Using Class Weights - No Differences.ipynb" (Changes weighting of algorithms from Model 4.4)
and "Model 4.5 - Model Using Patients Who Progress to Diabetes Within Five Years" and "Model 4.10 - Model Using Patients Who Progress to Diabetes Within 10 Years" 
and "Model 4.1 - 4.10 - Comparison of ALL Patient Cohorts.ipynb"

In all notebooks but last, we created models that contain only part of the total target group for patients who progress to diabetes. Each notebook specifies a condition that the
patients must be diagnosed with diabetes within one, two, three, four, five, or ten years, respectively, from the date when attending the assessment center initially.
These patient groups were created by using the ICD codes for patients diagnosed with diabetes, the epistart column which gives the date when the patient is diagnosed with 
diabetes, and the Date of attending assessment centre column which records the date when the patient begins his/her involvement in the study. We keep only prediabetic
patients at the beginning of their involvement in the study, and keep only patients who are confirmed to not progress to a diabetic state and patients who are 
diagnosed with diabetes after the start of the data collection within the time frame specified. 

Once receiving these patient groups, we follow the analysis of the notebook "All Models Matt Used Applied". We also included the calculation of feature importances for 
the machine learning algorithms Linear SVM, XGBoost, and Logistic regression. Our goal was to track the most important features as the time allowed for diabetes diagnosis of 
patients increased. This would have the potential for us to find features that are integral to diagnosis of patients in the near and far future, resulting in the creation of 
a robust model that generalizes well to new patients. 

Therefore, we look at the most common features between all the models to see which features have a large impact on all patients over time. 
We have found that HbA1c, Glucose, Waist Circumference among most common, and episodes containing diagnoses secondary ICD10 data found in every XGBoost top 3 important features.
With this information, we can begin to think about narrowing down which features we should keep for a final model that we suspect will receive nearly as good metrics 
as the models using all features. This will give us features that we may be able to use in clinical settings to help medical professionals classify prediabetic patients
who have an increased risk of developing diabetes.

The notebook "Model 4.1 - 4.10 - Comparison of ALL Patient Cohorts.ipynb" compares the patient demographics between all models involved.


18. Looking at Metrics For Model Using All Prediabetics With Assumption That All Prediabetic Patients Not Diagnosed With Diabetes Are Still Prediabetic
- Look at Notebook "Model 3 - Machine Learning Start Using Model With All Features keeping ALL prediabetic patients"

This model runs all algorithms with the patient groups being patients who we assume do not develop diabetes since they are not specified to vs. patients we know develop diabetes.
It does not perform as well as models 1, 2, or 2.1 and therefore we believe that we cannot assume that patients who are not marked as developing diabetes truly do not 
develop it. Later on, it will be our goal to classify these patients correctly.

19. Using Equal Cohort Sizes Without Assumption That All Prediabetic Patients Not Diagnosed With Diabetes Are Still Prediabetic - 
Look at Notebook "Model 5 - All Models Matt Used Applied With Equal Cohort Sizes"

In this notebook, we repeat all analyses from the notebook "All Models Matt Used Applied", but we take a random sample of 742 patients from the cohort of patients that progress
to a diabetic state so that the two cohort sizes are equal. Our goal is to reduce the biased prediction due to unequal sample sizes we have seen in earlier notebooks
when looking at the sensitivity, specificity, and confusion matrix of different machine learning algorithms.


20. Using Equal Cohort Sizes With Assumption That All Prediabetic Patients Not Diagnosed With Diabetes Are Still Prediabetic -
Look at Notebook "Model 3.1 - All Models Matt Used Applied Keeping ALL Prediabetics With Equal Cohort Sizes"

In this notebook, we follow in the footsteps of "Model 5 - All Models Matt Used Applied With Equal Cohort Sizes" but this time we use the patient cohort which assumes that 
all prediabetic patients who are not diagnosed with diabetes during the experiment remain prediabetic, even without explicit confirmation. We take a random sample of
4,261 patients from  the group of patients not progressing to diabetes so that we receive equal sample sizes. Again, this is done to reduce the bias of prediction 
we have seen when viewing the results of the sensitivity, specificity, and confusion matrix of different machine learning algorithms.


21. Incorporating Polygenic Risk Score
21a. Unzipping file in SCU

First, we received the file "for_nick_ethnic.txt.gz" from Scott and downloaded it. This file contains the polygenic risk scores for diabetic patients of different ethnicities
than caucasion. We then transferred it to the SCU and to unzip it we used the command:
gzip -d for_nick_ethnic.txt.gz

We then transferred the unzipped .txt file back to the Elemento_Research folder on the computer and imported it to jupyter notebook for incorporation into the model.


22a. Results of Using Total Amount of Polygenic Risk Scores - NEVER CREATED!!!!!!!!!!!!!

In this notebook, we apply all polygenic risk scores for diabetic patients. We first merge the two polygenic risk score dataframes together and then run the analysis as usual.


23. Comparing patients developing diabetes within different time periods to patients not developing diabetes - Look at notebooks 
"Model 7.1 - Patients who progress to a diabetic state within 1 to 2 years.ipynb" and "Model 7.2 - Patients who progress to a diabetic state within 2 to 3 years.ipynb" and 
"Model 7.3 - Patients who progress to a diabetic state within 3 to 4 years.ipynb" and "Model 7.4 - Patients who progress to a diabetic state within 4 to 5 years.ipynb" and 
"Model 7.5 - Patients who progress to a diabetic state within 5 to 10 years.ipynb"

In these notebooks, we run all the algorithms as in "Model 1 - All Models Matt Used Applied .ipynb" on the patients who develop diabetes within certain time periods after 
being classified as prediabetic at the time when they attended the assessment center. The goal of this is to discover if there are certain features that are important to 
the prediction of if a patient progresses to diabetes or not which are constant over time, or if there are changes in the types of features that become important for 
patients who develop diabetes at a later date. We continuously see that HbA1c is almost always the most important predictor of diabetic progression.


24. Comparing patients developing diabetes within certain time frames with patients developing diabetes within other certain time frames - Look at notebooks
"Models 8.1-8.5 comparing patients developing diabetes within first year with other time periods.ipynb" and
"Models 8.6-8.9 comparing patients developing diabetes within years 1 and 2 with other time periods.ipynb" and
"Models 8.10-8.12 comparing patients developing diabetes within years 2 and 3 with other time periods.ipynb" and
"Models 8.13-8.15 comparing patients developing diabetes within years 3 and 4 with other time periods and 4 to 5 years vs 5 to 10 years.ipynb"

In these notebooks, we are comparing the patients who develop diabetes within certain years with other patients who develop diabetes within a different year. 
The goal of this is similar to that of the 7.X models in that we want to discover which features are most important for developing diabetes during different time periods.
However, these models typically encompass only small amount of patients for each cohort, and therefore their scores are not very high and the features should not be 
interpreted as being confidently important to the prediction of the different time frames of developing diabetes patients. 


25a. Attempting to Classify Unknown Patients Into Progressing to Diabetes or Not Progressing to Diabetes PART 1 Using KNN - Look at notebooks below in order:
"Model 10 - K Nearest Neighbors Classifying Unknown Prediabetics to Diabetic Progression or Not.ipynb" and
"Model 10 - Demographic and Other Information for Different Cohorts Comparing Known and Unknown Prediabetics.ipynb" and
"Model 10 - Incorporating patients predicted to be prediabetic using KNN algorithm.ipynb" and
"Model 10 - Showing KNN Model Does Not Work.ipynb"

To attempt to classify the patients who are unknown to progress to diabetes or not, we create a K Nearest Neighbor Classifier in the first notebook. The reason for this was 
because we believed that the patients who have scores which would be closely similar to the known patients who progress to diabetes or do not progress to diabetes would be 
classified correctly themselves. By creating this model using the known patients, we then predicted the labels for the unknown patients. The model performed at its best for 
K = 19 neighbors on a validation set of patients who are known to progress to diabetes or not. Therefore, we used this value to predict the labels for the unknown patients for 
one of our models. This resulted in almost all patients being classified as progressing to diabetes. We believed that it may be better to use K = 1 neighbor instead because
this means that each unknown patient is predicted based on the known patient he/she is closest to. This classified about 2200 patients as not progressing to diabetes. We 
decided to use these patients to create a new model because this number of prediabetic patients is close to the actual number of prediabetic patients not progressing to diabetes
that has been observed in studies of betwee 10-30% of the total - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3891203/#:~:text=According%20to%20an%20ADA%20expert,prediabetes%20will%20eventually%20develop%20diabetes. 

The second notebook in the list above shows the demographic information for the patient cohorts for both K = 19 and K = 1 neighbor.

The third notebook contains the run of the model using the 742 known prediabetic patients not progressing to diabetes and the ~2200 patients predicted patients not to 
progress to diabetes vs. the 4261 patients known to progress to diabetes. The results of this model are quite good, but still could be due to an imbalance of patient 
size. Also, after talking with Dr. Elemento, he told me to validate that the KNN works well by using the unknown prediabetic patients and known diabetic progressing patients
as training data and predicting on the known prediabetic patients that don't progress to diabetes.

The fourth notebook predicts the labels for the known patients not progressing to diabetes using the unknown prediabetic patients and known diabetic progressing patients
as training data. The results of this predictor show a very low accuracy and therefore disprove the assumption that the unknown patients do not progress to diabetes. We also
showed that the 2200 patients we previously believed to be correctly classified not to progress to diabetes are also not good for predicting the true labels of the known 
prediabetic patients not progressing to diabetes. Therefore, we must think of a new way to classify the 12000 patients who are unknown to progress to diabetes or not.

25b. Attempting to Classify Unknown Patients Into Progressing to Diabetes or Not Progressing to Diabetes PART 2 Using ONLY Statistically Similar Patients - Look at notebooks 
"Model 10 - Failing to Find Statistical Difference Between Different Patient Groups"

This method does not work. We end up finding that the two cohorts, those who are known not to progress to diabetes and those that are known to progress to diabetes are 
extremely similar at the start of the study when their measurements are taken and they are classified as prediabetic.


26. NEW DEVELOPMENT IN FINDING PREDIABETIC PATIENTS - Using the SCU to find more prediabetic patients

After a discussion with Matt, he told me that there is an ICD code for prediabetic patients used, which is R73. We now want to incorporate these patients into our analysis.
First, we need to do as we did in Step 3. above and download the epistart column and make sure we have all eid numbers from these patients. These steps are shown below.

To import 'hesin_diag.txt':
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

To find all diabetes diagnosed patients:
prediabetes_only = hesin_diag[hesin_diag['diag_icd10'].str.contains('R73',na = False)]

To import 'hesin.txt'"
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

Next we cut down the number of columns in order to have a merge of only the columns we need from 'hesin.txt'"
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']]

We merge the dataframes together by eid and ins_index to get the final result with the dates of diagnosis:
patients_with_prediabetes_with_dates = prediabetes_only.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])

Finally we have to save this dataframe as a csv to be exported to my computer so that we can use it to compare the dates in this file to the dates of the other file 
in order to see if more patients in a prediabetic state progressed to a diabetic state:
patients_with_prediabetes_with_dates.to_csv(path_or_buf = '~nib4003/for_winscp/patients_with_prediabetes_with_dates')

* Notice how I have saved everything to a specific folder. I use this folder as a storage space to keep all files I move back and forth from my computer to the SCU
using WinSCP. If operating on a Windows computer, YOU SHOULD DOWNLOAD WinSCP found here: https://winscp.net/eng/download.php 
This tool makes transferring files from Windows to the SCU possible and very easy (drag and drop). 
Once the two csv files we created in step 3 are moved to the computer, upload them to Jupyter Notebook (what I did).
We then start the prediabetes classification and labeling of prediabetic patients who progressed to a diabetic state vs. those that never progressed to a diabetic state.


27. Prediabetic classification part 2 - Look at Notebooks "Model 11, 12 - Prediabetes Classification With Prediabetes ICD"

In this notebook, we repeated the analyses for the patients we already classified as known to progress to diabetes or not. We then performed the same analysis for the patients
with ICD codes for prediabetes. We classified these patients as progressing to diabetes if they had a designated ICD column for diabetes (E11) epistart date later than that
of the ICD column for prediabetes (R73). We also classified new patients who do not progress to diabetes. Since all the remaining patients were classified by medical 
professionals as having prediabetes, we assume that they were more closely monitored after their diagnosis. Therefore, we can assume that these patients never developed
diabetes, because if they did it would be recorded at a later date, as they were known to be susceptible. 


28. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 

Import pandas first:
import pandas as pd

First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_WITH_PREDIABETIC_ICD_CODES_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. 
The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (6982, 12711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only_WITH_PREDIABETIC_ICD_CODES')


29. Comparing the Demographic Makeup of the New Cohorts Using prediabetic Patients With ICD Specification - 
Look at notebook "Model 11, 12 - Demographic and Other Information for Different Cohorts"

In this notebook, we compare the demographics between the group of prediabetic patients who do not progress to diabetes and those that do progress to diabetes. We have done 
this same analysis before and are only repeating it for the new patients in our cohorts.


30. Feature Selection for New Cohorts Incorporating Prediabetic ICD Codes - Look at Notebook "Model 11 - Finding All Features For Machine Learning"

In this notebook, we repeat the procedures used previously to find the number of features that we want to keep from all features in total. The final number of features for
the cohorts using the prediabetic ICD codes is 250 features. After creation of this model, we label the patients for exportation to new notebooks for model usage.


31. Applying Machine Learning Algorithms Using the Features Found in the Notebook Above - Look at Notebook "Model 11 - All Models Matt Used Applied"

In this notebook, we keep the original cohort sizes using the prediabetic ICD codes and run all 11 machine learning algorithms to receive new metrics and look at important
features as we have done many times before. We receive very good metrics that do not seem as biased due to different class sizes like we see in previous models with less patients.


32. Applying Features From Literature to Patients Cohorts Incorporating Prediabetic ICD Codes - Look at Notebook "Model 12 - All Models Matt Used Applied On Features From Literature"

In this notebook, we take the features used in Model 2.1 and apply them to our new cohorts. Our results are better than model 2.1 and we find that the feature HbA1c
accounts for an enormous amount of the preiction for the patients.


33. Applying All Features From Notebook "Model 11 - Finding All Features For Machine Learning" to Equal Patient Cohort Sizes - 
Look at Notebook "Model 13 - All Models Matt Used Applied With Equal Cohort Sizes"

In this notebook, we repeated the steps of Model 11 but first took a random sample such that the cohort sizes were equal (n = 2322, due to addition of polygenic risk score).


34. Applying Machine Learning Algorithms to Features From Literature ONLY Used in Cornell EHR - 
Look at Notebook "Model 14 - Model For Features From Literature AND Cornell EHR Specific"

In this notebook, we used on measurements from model 12 which are also found in the Cornell EHR data. The reason for this is because we hope to implement our model into the 
Cornell EHR, which is less survey based than the UKBiobank, and so we keep primarily medical data features. This will allow us to make a more generalizable model for 
different healthcare systems that may not have access to survey data.


35. Applying ML Algorithms to All Features From "Model 11 - Finding All Features For Machine Learning" EXCEPT Hba1c and Glucose - 
Look at Notebook "Model 15 - All Models Matt Used Applied Without Hba1c or Glucose"

In this notebook, we use the same model as Model 11, but we cut out Hba1c and glucose. The reason for this is because these two features had a much greater feature importance
for all algorithms we looked at than all other features. As a result, we wanted to see how much their absence would hinder the prediction power of the model. We find that, as 
expected, the metrics all decrease, most substantialy the ROC AUC. 


36. Adding Features to ML Algorithms One at a Time - Look at Notebook "Model 16 - Model For Features From Literature AND Cornell EHR Specific ONE BY ONE by Importance"

In this notebook, we use the Linear SVM most important features from Model 11 to demonstrate the prediction power of the first few most important features from Model 11.
We make five models, starting with HbA1c and adding the next most important feature for the next model. The five most important features are HbA1c, glucose, BMI, 
Home area population density - urban or rural, and HDL Cholesterol. The results show that HbA1c has such a large impact on the prediction power of the model that the metrics
received using a Lienar SVM or Logistic Regression model with only HbA1c are almost the same as with the addition of the other features. It is worth noting that the 
specificity increases substantially with the addition of all 5 features as opposed to just HbA1c. Therefore, it is still important to add the other features so as to decrease
the bias of the model cohort sizes.


37. Creating Groups Based On Time Periods When Prediabetic Patients Who Progress to Diabetes Actually Progressed to Diabetic State -
Look at Notebook "Model 17 - Finding All Prediabetic ICD Code Patients Who Progress To Diabetes Within Different Time Intervals"

In this notebook, we create groups of all prediabetic patients who developed diabetes such that we have categories for those who develop diabetes within the first year of 
prediabetic diagnosis, within year one and year two, within year two and year three, within year three and year four, within year 


38. Attempting to Show Feature Differences Between Patients Who Develop Diabetes Within Different Time Periods Using Linear SVM - 
Look at Notebook "Model 17 - Looking At HbA1c Comparisons Over Years"


