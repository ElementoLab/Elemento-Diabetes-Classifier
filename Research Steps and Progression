Initial thoughts:

* IT MAY BE BEST TO DOWNLOAD THE COLUMNS ONTO MY COMPUTER THAT ARE USED TO DEFINE WHO HAS PREDIABETES AND WHO PROGRESSES TO DIABETES. 
IT SHOULD ONLY BE A FEW COLUMNS SO IT WILL NOT TAKE UP A LOT OF ROOM ON MY COMPUTER AND IT SHOULD BE QUICK COMPUTATIONALLY. 
THEN I CAN LOOK AT THE DATA ON A JUPYTER NOTEBOOK AND IT WILL BE MUCH EASIER TO DO WHAT I WANT.

# Procedure and Thinking Throughout the Project

1. Unzipping UKBiobank files for use in python or other language for data manipulation

BEFORE MANIPULATION OF THE DATA, WE FIRST HAVE TO OPEN THE .gz FILES USING LINUX COMMAND zcat FOR ALL FILES USED ALONG WITH USING THE > COMMAND TO MAKE THIS INTO A USABLE FILE. 
AN EXAMPLE CODE FOR ukb26867 IS SHOWN BELOW (for my path to the ukb26867 file in the SCU):
zcat /athena/elementolab/scratch/nib4003/ukbiobank/phenotypes/ukb26867.csv.gz > whole_file_26867

* This does take a little bit of time since the files are so big, but then we can use them in python
****** VERY IMPORTANT: We noticed differences in the counts across HTML files which contain the information regarding the same data 
(e.g. 20002-X.X occurs in whole_file_26867 and whole_file_33822). 
Scott said that this was most likely due to patients withdrawing their information from the UKBiobank over time, and therefore, 
as ethicial researchers we must use the columns with less patients in our analyses. 
Therefore, we have to be careful to take only the patients who want to be included in the study 
and we must make sure to use any duplicate column with the least number of patients in it.


2. Setting up Python on the Scientific Computing Unit (SCU)

Below we show how to use Spack to load Python. Before this is possible, follow this documentation page to set up all necessities for spack: https://wcmscu.atlassian.net/wiki/spaces/WIKI/pages/33594/Spack

Load python 3.7.0:
spack load -r python@3.7.0^gcc@6.3.0

Shows that python is loaded:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

Load pandas for use in python:
spack load -r py-pandas

Do not need to run this line of code but it shows that pandas is downloaded now:
echo $LOADEDMODULES | sed "s/:/\n/g" | sort

To open python (very simple):
python

Now we can import pandas as usual in the newly opened python file:
import pandas as pd


3. Use Scientific Computing Unit to create dataframe for exportation to personal computer in order to classify prediabetic patients

The next step is to read in the columns we want for our dataframe to be able to diagnose prediabetes. 
We have to do this separately for the HTML files we need to use to diagnose prediabetes. 
which we will add to the doctor diagnosis of diabetes due to Scott's figure saying self and doctor diagnosis are not statistically different for diabetes.
The first step here contains all the information for doctor diagnosis, age of diagnosis, gestational diabetics, and dates the blood was drawn.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867', usecols = ['eid', '74-0.0', '74-1.0', '74-2.0', '2443-0.0', '2443-1.0', '2443-2.0', '2976-0.0', '2976-1.0', '2976-2.0', '4041-0.0', '4041-1.0', '4041-2.0', '53-0.0', '53-1.0', '53-2.0'])

The second step here contains all the columns that contain the tests used to diagnose a prediabetic patient - HbA1c and Blood Glucose.
HbA1c is a standard measure for the classification of prediabetic patients, with patients scoring 42 mmol/mol - 47 mmol/mol in the prediabetic range.
Blood Glucose can also be used to classify prediabetic patients. Here we assume that the blood glucose levels are after fasting and therefore the range for prediabetic patients
for fasting blood glucose level is 5.6 mmol/L - 7.0 mmol/L. 
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385', usecols = ['eid', '30740-0.0', '30740-1.0', '30741-0.0', '30741-1.0', '30750-0.0', '30750-1.0', '30751-0.0', '30751-1.0'])

The third step here contains all the columns that correspond to a self-diagnosis of diabetes. 
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822', usecols = ['eid', '20002-0.0','20002-0.1', '20002-0.2', '20002-0.3', '20002-0.4', '20002-0.5', '20002-0.6', '20002-0.7', '20002-0.8', '20002-0.9', '20002-0.10', '20002-0.11', '20002-0.12', '20002-0.13', '20002-0.14', '20002-0.15', '20002-0.16', '20002-0.17', '20002-0.18', '20002-0.19', '20002-0.20', '20002-0.21', '20002-0.22', '20002-0.23', '20002-0.24', '20002-0.25', '20002-0.26', '20002-0.27', '20002-0.28', '20002-0.29', '20002-0.30', '20002-0.31', '20002-0.32', '20002-0.33', '20002-1.0', '20002-1.1', '20002-1.2', '20002-1.3', '20002-1.4', '20002-1.5', '20002-1.6', '20002-1.7', '20002-1.8', '20002-1.9', '20002-1.10','20002-1.11', '20002-1.12', '20002-1.13', '20002-1.14', '20002-1.15', '20002-1.16', '20002-1.17', '20002-1.18', '20002-1.19', '20002-1.20', '20002-1.21', '20002-1.22', '20002-1.23', '20002-1.24', '20002-1.25', '20002-1.26', '20002-1.27', '20002-1.28', '20002-1.29', '20002-1.30', '20002-1.31', '20002-1.32', '20002-1.33', '20002-2.0', '20002-2.1', '20002-2.2', '20002-2.3', '20002-2.4', '20002-2.5', '20002-2.6', '20002-2.7', '20002-2.8', '20002-2.9', '20002-2.10','20002-2.11', '20002-2.12', '20002-2.13', '20002-2.14', '20002-2.15', '20002-2.16', '20002-2.17', '20002-2.18', '20002-2.19', '20002-2.20', '20002-2.21', '20002-2.22', '20002-2.23', '20002-2.24', '20002-2.25', '20002-2.26', '20002-2.27', '20002-2.28', '20002-2.29', '20002-2.30', '20002-2.31', '20002-2.32', '20002-2.33'])

Now we can merge these dataframes by eid (patient number).
merged_prediabetes_information = first_step.merge(second_step, on = 'eid').merge(third_step, on = 'eid')

Finally we can write this dataframe to a csv so that we can import it to our desktop using winscp.
merged_prediabetes_information.to_csv(path_or_buf = '~nib4003/for_winscp/merged_prediabetes_information')

Another thing we must look into is a different way of finding patients who develop diabetes after their prediabetic classification. 
This involves using the hesin files located in /athena/elementolab/scratch/nib4003/ukbiobank/hesin which includes two files we will use named 'hesin.txt' and 'hesin_diag.txt'. 
'hesin.txt' contains the dates when a patient is diagnosed with a disease, in our case of course diabetes. 
'hesin_diag.txt' contains the actual diagnosis of diabetes for patients using the code E11, which includes all values from E110-E119. 
E11 is defined as the diagnosis for non-insulin dependent diabetes which is type 2 diabetes. This can be found in data-coding 19 in UDI 41202-0.0. 
We can combine these files together to create one dataframe that holds all diagnoses of diabetes and the dates in which these diagnoses were made. 
We then compare the dates to those of the ones of UDI 53-0.0 which is the 
'Date of attneding assessment centre' column which contains the dates we use to diagnose prediabetic patients since this is when their samples were taken. 
If the date using the hesin text files is later than that of the csv file for a prediabetic patient, then this patient developed diabetes. 
If the diabetes diagnosis is before the date of the csv file for a prediabetic patient, 
then the patient is already classified as diabetic and must be removed from the dataframe. 
The code to create the dataframe with all dates from the hesin files is shown below.
To import 'hesin_diag.txt':
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

To find all diabetes diagnosed patients:
type2_diabetes_only = hesin_diag[hesin_diag['diag_icd10'].str.contains('E11',na = False)]

To import 'hesin.txt'"
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

Next we cut down the number of columns in order to have a merge of only the columns we need from 'hesin.txt'"
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']]

We merge the dataframes together by eid and ins_index to get the final result with the dates of diagnosis:
patients_with_type2_diabetes_with_dates = type2_diabetes_only.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])

Finally we have to save this dataframe as a csv to be exported to my computer so that we can use it to compare the dates in this file to the dates of the other file 
in order to see if more patients in a prediabetic state progressed to a diabetic state:
patients_with_type2_diabetes_with_dates.to_csv(path_or_buf = '~nib4003/for_winscp/patients_with_type2_diabetes_with_
dates')

* Notice how I have saved everything to a specific folder. I use this folder as a storage space to keep all files I move back and forth from my computer to the SCU
using WinSCP. If operating on a Windows computer, YOU SHOULD DOWNLOAD WinSCP found here: https://winscp.net/eng/download.php 
This tool makes transferring files from Windows to the SCU possible and very easy (drag and drop). 
Once the two csv files we created in step 3 are moved to the computer, upload them to Jupyter Notebook (what I did).
We then start the prediabetes classification and labeling of prediabetic patients who progressed to a diabetic state vs. those that never progressed to a diabetic state.

Below are the columns we kept in step 3 csv files:

In HTML: ukb42385.csv.gz

Glucose - 30740-0.0
Glucose - 30740-1.0
Glucose assay date - 30741-0.0
Glucose assay date - 30741.1.0
Glycated haemoglobin (HbA1c) - 30750-0.0
Glycated haemoglobin (HbA1c) - 30750-1.0
Glycated haemoglobin (HbA1c) assay date - 30751.0.0
Glycated haemoglobin (HbA1c) assay date - 30751.1.0
Date of attending assessment centre - 53-0.0
Date of attending assessment centre - 53-1.0
Date of attending assessment centre - 53-2.0

In HTML: ukb26867.csv.gz

Diabetes diagnosed by doctor - 2443-0.0
Diabetes diagnosed by doctor - 2443-1.0
Diabetes diagnosed by doctor - 2443-2.0
Age diabetes diagnosed - 2976-0.0
Age diabetes diagnosed - 2976-1.0
Age diabetes diagnosed - 2976-2.0
Gestational diabetes only - 4041-0.0
Gestational diabetes only - 4041-1.0
Gestational diabetes only - 4041-2.0
Fasting time - 74-0.0

In HTML: ukb33822.csv.gz

Non-cancer illness code, self-reported - Use all 20002-0.0 to 20002-2.33 (99 columns in total)


4. Prediabetic Classification and Progression to Diabetes Labeling - Look at Jupyter Notebook file "Prediabetes Classification.ipynb"

To begin, it would be most efficient to focus on finding the certain patients that will be involved in this study so that 
we do not have to waste time waiting for computations on many pateints to run initially. 
We do this using two measurements. First, we keep patients with an HbA1c measurement in the range 42 mmol/mol - 47 mmol/mol
Second, we use blood glucose to measure if a patient has prediabetes for patients who took a fasting blood sugar test.
For patients who fasted 8 or more hours, prediabetes based on blood glucose level lies in the range 5.6 mmol/L - 7.0 mmol/L
For patients who did not fast at least 8 hours, this test does not have a specified range for prediabetic patients, and therefore cannot be used.
We removed all doctor diagnosed diabetic patients who fall into our prediabetic ranges for the HbA1c and blood glucose measurements 
and also patients who have gestational diabetes only because these patients do not develop diabetes in the way we desire and may bias our model.

For our analysis, we assume that self-diagnosis of diabetes is equivalent to true diagnosis of diabetes.
Therefore, we need to take into account patients which we classify as prediabetic, but who self-diagnose themselves as diabetic at the start of the study.
These patients must be cut from the dataframe because we assume that they begin the study with diabetes, not prediabetes.
We also need to consider prediabetic patients who are never diagnosed with diabetes (through the doctor diagnosis columns 2443-X.X or through the ICD values),
but do self-diagnose themselves with diabetes. We need to keep these patients in the dataframe and classify them as prediabetes who progress to diabetes.

We use two sequential ideas to create the group of patients who will be kept for our classifier for non-self-diagnosis patients and how to label them:

The first is to use the ICD column in which prediabetic patients are diagnosed with diabetes after their initial prediabetic classification. 
This is done by comparing the "Date of attending assessment centre" (53-0.0) column from the ukb42385 csv file with the "epistart" column from the "hesin.txt" file.
Prediabetic classified patients with a "Date of attending assessment centre" before the "epistart" date are labeled as developing diabetes.
Prediabetic classified patients with an "epistart" date before the "Date of attending assessment centre" have diabetes before the actual prediabetic classification,
and must be cut from the patients we classified as prediabetic because they are truly classified as diabetics. 
Once we have found all patients who are shown to develop diabetes from the ICD codes 
(E110-E119 for Type 2 Diabetes shown here for Non-insulin-dependent diabetes mellitus - https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=19&nl=1),
we cut them from the total dataframe of prediabetic patients since we already know they are to be labeled as progressing to diabetes.

The second idea is to then filter through the remaining prediabetic patients 
and keep only the patients who we know either developed diabetes or did not develop diabetes at a later date.
To do this, we only keep prediabetic patients in our study that contain values in columns in which a doctor diagnosis whether a patient has diabetes or not 
at a later date when a patient returns and is re-evaluated (2443-1.0 and 2443-2.0). We must cut all the prediabetic classified patients from the final dataframe 
who do not have a confirmational value of whether or not they developed diabetes. We can then label the patients as developing diabetes 
if they begin classified as prediabetic (2443-0.0) but later are diagnosed with diabetes (2443-1.0 or 2443-2.0).

These two ideas allow us to label all our targets as progressing to diabetes or not. We then take a list of all eid numbers for prediabetic patients progressing 
to a diabetic state and label them. We also do the same for the remaining prediabetic patients who did not progress to a diabetic state.
We can then save our final dataframe to a csv and export it to the SCU using WinSCP where we begin the next step.


5. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 
First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
less_features_first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (5003, 12711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only')


6. Keeping Features For Final Model By Inspection - Look at Jupyter Notebook file "Machine Learning Start Using Model With All Features.ipynb"

We now have a dataframe containing all 5003 patients who began with prediabetes and either progressed to a diabetic state (4261) or did not progress to diabetes (742) 
who are described by a total of 12711 features. We notice that there are duplicate columns within the UKBiobank data which may be due to patients deciding to withdraw
their information from the database after their initial measurements. To be ethical researchers, we keep the columns which contain less information so as to not exploit
these patients. 

We then cut all features from the dataframe which contain only NaN values since these will not give any information for classification. This totals 3224 columns.
The next step in the pipeline is to cut all dates out of the dataframe. Since we trying to predict the patients who will and will not progress to a diabetic state
based on their initial information, the dates involved do not provide the model with information about the outcome of the patient. 
There are 54 different dates in the dataframe, some containing multiple entries. 

The next thing we do is look at the first column for each feature (those that end in -0.0) for a large proportion of NaN values. 
The reason we do this is because these columns mostly contain more information than the next numerical columns (-0.1 or -1.0) 
and therefore if there are many NaN values in the -0.0 columns then there will not be enough information in total to have any weight on our model.
We use a criteria of 80% NaN values or greater in a column to drop it from the dataframe. This results in a dataframe with 3901 columns.

Our next step is to understand what all remaining columns represent and filter out those that are not important.
The dataframe is composed of 3140 floats, 6 integers, and 755 objects.
We begin our filtering with the object features. All features are labeled in the .ipynb notebook and labeled as to whether or not they will be kept for the next steps.
We then drop columns which we believe have no relevance to the model's predictive ability. This results in a total of 3366 columns.
We next look at the integer features. We again show the features in the .ipynb notebook and decide which to keep. We end up keeping e of the 6 integer columns (eid included).
We finally look at the float features. In the .ipynb notebook we decide which to keep in our final model by expecting them in the HTML and on the UKBiobank. 
After dropping the unnecessary float features, we are left with a dataframe with 1373 columns.

Since we are to only focus on the initial data taken from the prediabetic patients, we drop all columns that do not end in X-0.X as these represent the recordings from
the first visit to the assessment center by the patients. 
However, we notice that there are some columns that are represented by one measurement in the form X-2.0, which we keep in the dataframe as well. 
The resulting number of features remaining is 899.

At this point, we further cut down on features with many NaN values. All columns with 30% or greater NaN values are dropped from the dataframe. The result is 217 columns.
The makeup of these data types is 205 floats, 3 integer, and 9 objects. 

Next we focus our attention on the remaining object columns. We believe that the only way to get information from these columns is to one hot encode the data 
and we do not believe this will actually result in a greater prediction power of the model because there are numerous values that are taken by all patients.
Values are not common among patients so the result would just be the creation of an enormous amount of columns that contain no information that helps us. 
Therefore, we drop the object columns from the dataframe.

We again label all remaining integer and float features. We find that some of our floats are categorical in nature and therefore we must one hot encode these columns using 
pandas get_dummies. The NaN values remain in these columns when using this function.

We notice that some of the float columns contain negatives, which our model does not allow for. 
We therefore need to make a coding for these patients which rectifies the situation. All codings for all categorical floats are found in the .ipynb. 
When we set the values of -1 and -3 to NaN, these will be filled in like any other NaN value with the median of the column.

Next, we fill all NaN values in the remaining float columns with the median of each column, so as to not bias the center of each data distribution.

We then read in the dataframe with each patient labeled correctly from "Prediabetes Classification.ipynb" and merge the target column with our current dataframe.

After further inspection, we find that there are columns in our dataframe which contain only 0 values and therefore will not increase the model's prediction power.
Therefore, we cut these columns from the dataframe, resulting in a dataframe with 479 columns after one hot encoding.

Next, we normalize all remaining float columns. First, for all columns containing negative values, we subtract the minimum from all values (resulting in adding the 
absolute value of the most negative number), resulting in only positive numbers. We then normalized all float columns by dividing each column by its maximum
value, effectively resulting in the range of each column to be between 0 and 1. 

We then saved this dataframe in csv format to be used in another notebook for further feature selection and machine learning model creation.


7. Creation of a dataframe using "Episodes" features from ukb26867 and Analysis of Model - Look at "Machine Learning Start Using Model With All Features and Feature Selection"
and "Machine Learning After Inspection of All Features INCLUDING Episodes" and "Machine Learning After Inspection of All Features" (contains models without Episodes features 
even though this notebook has been updated such that the analysis shows the addition of the Episodes columns before the models constructed at the end of the notebook) and
"Parameter Tuning for Model With Episodes Features"

We have gone back to the model and found that there are columns which begin with "Episodes" which we want to keep in the model because we believe they contain 
information that can help the predictive power of the model. We redid the analysis in the "Machine Learning After Inspection of All Features" notebook and saved 
this dataframe as 'prediabetes_df_after_data_manipulation_ready_for_feature_selection_with_episodes_data' to distinguish it from the dataframe we saved without the episodes
columns entitled 'prediabetes_df_after_data_manipulation_ready_for_feature_selection'. In the "Machine Learning After Inspection of All Features" notebook, the models at 
the end of this notebook use the 'prediabetes_df_after_data_manipulation_ready_for_feature_selection' dataframe, not the updated one that was created in this notebook. 
In the notebook entited "Machine Learning After Inspection of All Features INCLUDING Episodes", we have made models using the 
'prediabetes_df_after_data_manipulation_ready_for_feature_selection_with_episodes_data' dataframe and have shown that these features increase the AUC ROC when incorporated.

The notebook "Parameter Tuning for Model With Episodes Features" contains a section in which we attempt to change the parameters of the Random Forest Classifier in order 
to receive better metrics. We do this by changing the values of many of the hyperparameters for the model and observing the changes in the ROC AUC and confusion matrices
for cross fold validation. Our results show that the default parameters seem to work optimally for our model.


8a. Adding ICD Codes Step 1 - Getting ICD Codes for Prediabetic Patients From SCU

One thing we want to try to incorporate into the model is all the different diseases every prediabetic patient has. One way in which we believe this is possible is to make 
a count of all the different diagnoses for a patient and use this as an extra feature. To do so, we must use the hesin.txt and hesin_diag.txt files again.
Our commands are very similar to those described in 3., only this time we are keeping all diagnoses instead of just type 2 diabetes. The commands used are shown below.
* Note we will be using python so use the information above in 2. to open python for the following use.

First, we import the hesin_diag.txt file:
hesin_diag = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin_diag.txt', sep = '\t')

Next, we keep only the columns that will help us make the counts of diagnoses for each patient:
necessary_columns_for_diagnosis = hesin_diag[['eid', 'ins_index', 'diag_icd9', 'diag_icd10']] 

The next step is to import the hesin.txt file:
hesin_txt = pd.read_csv('/athena/elementolab/scratch/nib4003/ukbiobank/hesin/hesin.txt', sep = '\t')

We then cut down the columns for this file to those that we want to keep:
hesin_txt_for_merge = hesin_txt[['eid', 'ins_index', 'epistart']] 

Next we merge the two dataframes together by eid and ins_index to match the correct patients with the correct disease:
all_patients_and_their_diagnoses = necessary_columns_for_diagnosis.merge(hesin_txt_for_merge, on = ['eid', 'ins_index'])  

Since this results in a large number of rows, we want to keep only the prediabetic patient results. Therefore, we import our prediabetic dataframe.
all_prediabetes = pd.read_csv('/home/nib4003/for_winscp/prediabetic_all_possible_to_classify_final')

We have to drop an unncessary column that got dragged along:
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Below we make a list of the eid numbers for prediabetic patients:
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we can keep only the prediabetic patients from the total dataframe:
all_diagnoses_only_prediabetics_at_start = all_patients_and_their_diagnoses[all_patients_and_their_diagnoses.eid.isin(dfToList_all_prediabetes)]

Finally, we save this dataframe for transfer to the SCU:
all_diagnoses_only_prediabetics_at_start.to_csv(path_or_buf = '~nib4003/for_winscp/only_prediabetes_patients_all_diagnoses_for_every_disease')


8b. Adding ICD Codes Step 2 - Applying ICD Codes To Prediabetic Patients - Look at "ICD Column Incorporation" Notebook

In this notebook, we add the ICD codes to our model which begins using all features of the total dataframe in order to understand how multiple different diagnoses can 
effect a patient. We want to see if more diagnoses increases the prediction power of the model that a prediabetic patient will or will not progress to a diabetic state. 


9. Prediabetic Classification and Progression to Diabetes Labeling With Assumption That All Patients Who Have Diabetes Are Explicitly Labeled (Using ICD Codes Or Diabetes
Diagnosed by Doctor Columns Directly From UKBiobank) - Look at Jupyter Notebook file "Prediabetic Classification keeping ALL Prediabetic Patients.ipynb"

To begin, it would be most efficient to focus on finding the certain patients that will be involved in this study so that 
we do not have to waste time waiting for computations on many pateints to run initially. 
We do this using two measurements. First, we keep patients with an HbA1c measurement in the range 42 mmol/mol - 47 mmol/mol
Second, we use blood glucose to measure if a patient has prediabetes for patients who took a fasting blood sugar test.
For patients who fasted 8 or more hours, prediabetes based on blood glucose level lies in the range 5.6 mmol/L - 7.0 mmol/L
For patients who did not fast at least 8 hours, this test does not have a specified range for prediabetic patients, and therefore cannot be used.
We removed all doctor diagnosed diabetic patients who fall into our prediabetic ranges for the HbA1c and blood glucose measurements 
and also patients who have gestational diabetes only because these patients do not develop diabetes in the way we desire and may bias our model.

For our analysis, we assume that self-diagnosis of diabetes is equivalent to true diagnosis of diabetes.
Therefore, we need to take into account patients which we classify as prediabetic, but who self-diagnose themselves as diabetic at the start of the study.
These patients must be cut from the dataframe because we assume that they begin the study with diabetes, not prediabetes.
We also need to consider prediabetic patients who are never diagnosed with diabetes (through the doctor diagnosis columns 2443-X.X or through the ICD values),
but do self-diagnose themselves with diabetes. We need to keep these patients in the dataframe and classify them as prediabetes who progress to diabetes.

We use two sequential ideas to create the group of patients who will be kept for our classifier for non-self-diagnosis patients and how to label them:

The first is to use the ICD column in which prediabetic patients are diagnosed with diabetes after their initial prediabetic classification. 
This is done by comparing the "Date of attending assessment centre" (53-0.0) column from the ukb42385 csv file with the "epistart" column from the "hesin.txt" file.
Prediabetic classified patients with a "Date of attending assessment centre" before the "epistart" date are labeled as developing diabetes.
Prediabetic classified patients with an "epistart" date before the "Date of attending assessment centre" have diabetes before the actual prediabetic classification,
and must be cut from the patients we classified as prediabetic because they are truly classified as diabetics. 
Once we have found all patients who are shown to develop diabetes from the ICD codes 
(E110-E119 for Type 2 Diabetes shown here for Non-insulin-dependent diabetes mellitus - https://biobank.ndph.ox.ac.uk/showcase/coding.cgi?id=19&nl=1),
we cut them from the total dataframe of prediabetic patients since we already know they are to be labeled as progressing to diabetes.

The second idea is to then filter through the remaining prediabetic patients 
and keep only the patients who we know either developed diabetes or did not develop diabetes at a later date.
To do this, we only keep prediabetic patients in our study that contain values in columns in which a doctor diagnosis whether a patient has diabetes or not 
at a later date when a patient returns and is re-evaluated (2443-1.0 and 2443-2.0). We must cut all the prediabetic classified patients from the final dataframe 
who do not have a confirmational value of whether or not they developed diabetes. We can then label the patients as developing diabetes 
if they begin classified as prediabetic (2443-0.0) but later are diagnosed with diabetes (2443-1.0 or 2443-2.0).

These two ideas allow us to label all our targets as progressing to diabetes or not. We then take a list of all eid numbers for prediabetic patients progressing 
to a diabetic state and label them. We also do the same for the remaining prediabetic patients who did not progress to a diabetic state.
We can then save our final dataframe to a csv and export it to the SCU using WinSCP where we begin the next step.


10. Using the SCU to create a dataframe for all the remaining patients that contains all the features from the UKBiobank to be used for feature selection for the sample
of patients which we assume do not progress to diabetes if not explicitly stated through ICD or doctor diagnosis of diabetes from UKBiobank columns directly.

The next step is to make a classifier that contains many columns that we want to cut down using feature selection. 
We first start by using Linux commands to create our 4 files that contain all the HTML features (whole_file26867, whole_file_33822, whole_file_41972, whole_file_42385). 
We next need to combine all the features with the dataframe we created of all classified prediabetic patients so that the computation does not take a long time. 
First, we import our prediabetic patient dataframe.
all_prediabetes = pd.read_csv('for_winscp/prediabetic_all_possible_to_classify_keeping_all_prediabetic_patients_final')

An additional column comes along with the above dataframe that we need to drop, shown below.
all_prediabetes = all_prediabetes.drop(columns = ['Unnamed: 0'])

Next we make a list of the eid values of the prediabetic patients at the start of the study.
dfToList_all_prediabetes = all_prediabetes['eid'].tolist()

Next we need to import the four large files with all the features. An example is shown below but we need to do this for all the files. 
This does take a long time for every file since they are so large.
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')

We can look at the new number of features using the command below.
less_features_first_step.shape

The complete list of commands for all four files is shown below (includes the previous 3):
first_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_26867')
first_step.shape
second_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_33822')
second_step.shape
third_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_41972')
third_step.shape
fourth_step = pd.read_csv('/athena/elementolab/scratch/nib4003/documentation_files/whole_file_42385')
fourth_step.shape

We then keep only the patients from the first_step dataframe that match our eid numbers for the prediabetic patients.
first_step_only_prediabetics_at_start = first_step[first_step.eid.isin(dfToList_all_prediabetes)]

Fianlly we merge all the files together to get one dataframe with all the features which only contains the patients which we are able to classify into developing diabetes or not. The merge command is shown below.
all_features_prediabetics_only = first_step_only_prediabetics_at_start.merge(second_step, on = 'eid').merge(third_step, on = 'eid').merge(fourth_step, on = 'eid')

If we run all_features_prediabetes_only.shape we see that the shape is (17,200, 12,711) which is the number of patients we found and the total number of features.

Now we finally have the whole dataframe with all the features for the prediabetic patients. 
The next step is to save the dataframe as a csv, as we have done before, in order to send it back to the desktop using WinSCP.
all_features_prediabetics_only.to_csv(path_or_buf = '~nib4003/for_winscp/all_features_prediabetics_only_keeping_all_prediabetic_patients')

*Notice the only change between this and step 5. is the importing of the correct dataframe to being and the export of the final dataframe with a new name including 
"keeping_all_prediabetic_".


11. Testing Model Created Using the Sample Of Patients Which We Assume Do Not Progress to Diabetes If Not Explicitly Stated Through ICD Or 
Doctor Diagnosis Of Diabetes From UKBiobank Columns Directly - Look at Notebook "Machine Learning Start Using Model With All Features keeping ALL prediabetic patients.ipynb"

In this notebook, we run through the same analysis previously performed in "Machine Learning Start Using Model With All Features.ipynb" but this time we had a larger 
sample size due to our assumption that prediabetic patients who are not classified as diabetic remain prediabetic. We show in this notebook that even though we have a much
larger sample size, our results are actually worse than the results without the assumption. Therefore, we find that more data is not as good as quality data. 
We do not further our analysis with this assumption due to its worse performance. 


12. Testing Using Many Machine Learning Algorithms - Look at Notebook "All Models Matt Used Applied"

After Matt Wickersham's great presentation, we decided to follow his steps and use all ML algorithms he used for his classification of alcohol usage. We also added the 
algorithms Complement Naive Bayes, Linear SVM, and Easy Ensemble. 

In this notebook, we use stratified k fold cross validation, due to the imabalance between our cohort group sample sizes, with the number of splits equal to 10. 
For all ML algorithms, we calculate the accuracy, area under the receiver operating characteristic curve, area under the precision recall curve, f1 scores, 
and all corresponding standard deviations to show as metrics for the classification of prediabetic patients progressing to a diabetic state. 
We also plot an ROC AUC curve corresponding to every algorithm together to compare the results of the different models for this metric.
We also plot a precision recall curve corresponding to every algorithm together to compare the results of the different models for this metric.
Tables of these values have been created using Word to show all results in the same place. 


13. Incorporating New Features Using ICD Codes for Patients With Hypertension or Obesity and The Number of Diseases - Look at Notebook "ICD Column Incorporation.ipynb"

In this notebook, we use the ICD10 column from the hesin files in the SCU to create two features which we add to the model which was created using all features to begin. 

1. The first feature is the number of diseases a patient has before their initial measurments at the assessment center. We count all the diseases a patient is diagnosed
with that have an epistart date before the date of attending assessment center column. We then add these values into the overall dataframe with all the features.
For patients who do not have a reported medication being taken, we give them a value of 0.

2. The second feature is the labeling of whether a patient had obesity and/or hypertension before the initial measurements were taken, once again comparing the epistart 
and date of attending assessment center columns. We believe that the prediabetic patients who had obesity and/or hypertension would most likely develop diabetes and 
would provide the model with a better prediction power. 

Unfortunately, after incorporation of these features into our model, we found that their feature importances were extremely low and had no impact on the model. Therefore,
we decide not to use them as features in future models.


14. Running Statistical Tests Between Different Cohort Features - Look at Notebook "Demographic and Other Information for Different Cohorts"

In this notebook, we recreated Matt Wickersham's tables for his project which contain different demographic and medical information for the two cohorts. 
We calculate the average values of many features in the model, and run Levene's test on all continous variables first to determine if the variance of the two groups differ
in respect to any of the features. We find that this is the case only for the age of the patients between the two cohorts. We then used independent t-tests to statistically
show if there is a significant difference between the means of the features we included for cohorts. We discovered that the only feature with a statistically significant 
difference between the means is Age. 


15. Comparison of Model With and Without HbA1c and Glucose Features - Look at Notebooks "ML Model Feature Selection WITH HBA1C and GLUCOSE.ipynb" and "Feature Selection and Machine Learning After Inspection of All Features.ipynb"

The first notebook contains a model with HbA1c and Glucose, while the second notebook does not contain these features. We see that the model that does contain them 
performs better, and HbA1c is even a top feature. We believe that even though we used this to define our patients who are prediabetic, this feature is still necessary 
to keep in the model and there is no bias resulting from leaving it in the model.


16. Creation of New Model By Adding New Features From Literature to Previous Model With Only Features From Literature - Look at 
Notebook "Machine Learning Start Features From Literature With More Features Added" - 
contains additional features from https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-019-0918-5

In this notebook, we add new features from the source above. This paper used machine learning models to predict prediabetic patients from non-prediabetic patients.
Although this is not the same task as us, they provide what could be used as a baseline for comparison with our model's metrics. This paper also discusses other papers and 
lists some of the features from those publications, which this lab group also inclued in their models. 

We show that with the inclusion of the new features in our model, the ROC AUC is greater than that of the first model created without the features. However, the performance
is not as good as the model which began using all features. We also repeat the analysis in "All Models Matt Used Applied " to get the metrics for all machine learning 
algorithms. A table was created which contains the accuracy, AUC ROC, AUPRC, F1 score, and the uncertainties for all metrics for classifying prediabetic patients 
progressing to a diabetic state.


17. Creation of Model Using Patients Who Progress To A Diabetic State WIthin Certain Time Frames Of Prediabetic Classification Based On Their Attending Assessment Center Date -
Look at Notebboks "Model Using Patients Who Progress to Diabetes Within One Year" and "Model Using Patients Who Progress to Diabetes Within Two Years"
and "Model Using Patients Who Progress to Diabetes Within Three Years" and "Model Using Patients Who Progress to Diabetes Within Four Years" 
and "Model Using Patients Who Progress to Diabetes Within Five Years" and "Model Using Patients Who Progress to Diabetes Within 10 Years"

In these notebooks, we created models that contain only part of the total target group for patients who progress to diabetes. Each notebook specifies a condition that the
patients must be diagnosed with diabetes within one, two, three, four, five, or ten years, respectively, from the date when attending the assessment center initially.
These patient groups were created by using the ICD codes for patients diagnosed with diabetes, the epistart column which gives the date when the patient is diagnosed with 
diabetes, and the Date of attending assessment centre column which records the date when the patient begins his/her involvement in the study. We keep only prediabetic
patients at the beginning of their involvement in the study, and keep only patients who are confirmed to not progress to a diabetic state and patients who are 
diagnosed with diabetes after the start of the data collection within the time frame specified. 

Once receiving these patient groups, we follow the analysis of the notebook "All Models Matt Used Applied". We also included the calculation of feature importances for 
the machine learning algorithms Linear SVM, XGBoost, and Logistic regression. Our goal was to track the most important features as the time allowed for diabetes diagnosis of 
patients increased. This would have the potential for us to find features that are integral to diagnosis of patients in the near and far future, resulting in the creation of 
a robust model that generalizes well to new patients. 

Therefore, we look at the most common features between all the models to see which features have a large impact on all patients over time. 
We have found that HbA1c, Glucose, Waist Circumference among most common, and episodes containing diagnoses secondary ICD10 data found in every XGBoost top 3 important features.
With this information, we can begin to think about narrowing down which features we should keep for a final model that we suspect will receive nearly as good metrics 
as the models using all features. This will give us features that we may be able to use in clinical settings to help medical professionals classify prediabetic patients
who have an increased risk of developing diabetes.


18. Looking at Metrics For Model Using All Prediabetics With Assumption That All Prediabetic Patients Not Diagnosed With Diabetes Are Still Prediabetic
- Look at Notebook "Machine Learning Start Using Model With All Features keeping ALL prediabetic patients"





